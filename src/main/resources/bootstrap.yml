server:
  port: 9898
  tomcat:
    basedir: ./temp/

spring:
  application:
    name: mqtt-demo
  datasource:
    url: jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&characterEncoding=UTF-8&allowMultiQueries=true&useJDBCCompliantTimezoneShift=true&useLegacyDatetimeCode=false&serverTimezone=GMT%2b8&useSSL=false
    driver-class-name: com.mysql.cj.jdbc.Driver
    type: com.zaxxer.hikari.HikariDataSource
    username: root
    password: root
  liquibase:
    enabled: false
  servlet:
    multipart:
      max-file-size: 500MB
      max-request-size: 1000MB
  cloud:
    discovery:
      metadata:
        env: 111
    nacos:
      username: nacos
      password: nacos
      discovery:
        #server-addr: 172.31.200.12:8847,172.31.200.12:8848,172.31.200.11:8848
        #server-addr: 127.0.0.1:8848
        server-addr: 127.0.0.1:8848
        service: ${spring.application.name}
        group: DEFAULT_GROUP
        namespace:
        enabled: false
      config:
        group: DEFAULT_GROUP
        namespace:
        #server-addr: 172.31.200.12:8847,172.31.200.12:8848,172.31.200.11:8848
        #server-addr: 127.0.0.1:8848
        server-addr: 127.0.0.1:8848
        file-extension: yml
        name: ${spring.application.name}
        enabled: false
  kafka:
    enabled: false
    bootstrap-servers: 127.0.0.1:9092
    producer:
      # 发生错误后，消息重发的次数。
      retries: 1
      #当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算。
      batch-size: 16384
      # 设置生产者内存缓冲区的大小。
      buffer-memory: 33554432
      # 键的序列化方式
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      # 值的序列化方式
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      # acks=0 ： 生产者在成功写入消息之前不会等待任何来自服务器的响应。
      # acks=1 ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应。
      # acks=all ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。
      acks: 1
    consumer:
      group-id: test-consumer-group
      # 自动提交的时间间隔 在spring boot 2.X 版本中这里采用的是值的类型为Duration 需要符合特定的格式，如1S,1M,2H,5D
      auto-commit-interval: 1S
      # 该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：
      # latest（默认值）在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的记录）
      # earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录
      auto-offset-reset: earliest
      # 是否自动提交偏移量，默认值是true,为了避免出现重复数据和数据丢失，可以把它设置为false,然后手动提交偏移量
      enable-auto-commit: false
      # 键的反序列化方式
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      # 值的反序列化方式
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      # 批量消费一次最大拉取的数据量
      max-poll-records: 2
    listener:
      # 在侦听器容器中运行的线程数。
      concurrency: 3
      type: batch
      #listner负责ack，每调用一次，就立即commit
      ack-mode: manual_immediate
      missing-topics-fatal: false

customer:
  kafka:
    consumer:
      listener:
        enable:
          topic-test1: false

# mybatis-plus
mybatis-plus:
  mapper-locations: classpath://mybatis/mapper/*.xml
  typeAliasesPackage: com.iflytek.zhunzhong.demo.pojo.entity
  global-config.banner: false
  #global-config.db-config.id-type: ASSIGN_ID

groovy:
  classPath: xx

mqtt:
  emqx:
    broker: tcp://127.0.0.1:1883
    pubClientId: emqx_pub_test
    subClientId: emqx_sub_test
    pubTopic: /testtopic/1
    subTopic: /testtopic/#
    userName: test
    passWord: test


management:
  endpoints:
    web:
      exposure:
        include: "*"
  endpoint:
    health:
      show-details: always

